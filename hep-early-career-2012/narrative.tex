\documentclass[12pt]{article}

\usepackage{graphicx}

\usepackage[margin=1in,dvips]{geometry}

\usepackage{deluxetable}

\newcommand{\commissdate}{Fall 2012}
\newcommand{\surveyproper}{Spring 2013}
\newcommand{\devauc}{De Vaucouleurs'}
\newcommand{\devprof}{exp$(-r^{1/4})$}
\newcommand{\overhead}{10\%}
\newcommand{\speedupnum}{10}
\newcommand{\speedup}{ten}
\newcommand{\overallspeedup}{170}
\newcommand{\sncut}{20}

\newcommand{\snsize}{(S/N)$_{T}$}

% Get rid of the normal page numbers
%\usepackage{nopageno}

% Now use fancyhdr but only to clear the headers and put a page number in the
% upper right

%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\rhead{}
%\rfoot{}
%\lfoot{}
%\cfoot{\thepage}
%\renewcommand{\headrulewidth}{0.0pt}

% for comments
\usepackage{verbatim}


% bibliography stuff
\usepackage{natbib}
\input{aasdef.tex}

\begin{document}

\setcounter{page}{0}
\addcontentsline{toc}{section}{Cover Page}
%\addcontentsline{toc}{section}{Table of Contents}

% Title and TOC
%\newpage 

\begin{center}
 {\large {\bf Cover Page}}
\end{center}

\vspace{3mm}
\noindent
\begin{center}
{\bf LAB 12-751 Early Career:  \\ Measuring Dark Energy with Gravitational \\
Lensing in the Dark Energy Survey}
\end{center}

\vspace{3mm}
\noindent
{\bf DOE National Laboratory:} Brookhaven National Laboratory

\vspace{3mm}
\noindent
{\bf Street Address:} PO Box 5000, Upton, NY 11973

\vspace{3mm}
\noindent
{\bf Principal Investigator (PI):} Erin Sheldon

\vspace{3mm}
\noindent
{\bf Position Title of PI:} Physicist

\vspace{3mm}
\noindent
{\bf Business Mailing Address of PI:} Bldg 510, Brookhaven National Laboratory, Upton, NY
11973

\vspace{3mm}
\noindent
{\bf Telephone Number of PI:} (631) 344-3117

\vspace{3mm}
\noindent
{\bf Email of PI:} erin.sheldon@gmail.com

\noindent
{\bf Program Announcement: }LAB 12-751

\vspace{3mm}
\noindent
{\bf DOE/Office of Science Program Office:}  HEP

\vspace{3mm}
\noindent
{\bf Topic Area:} Cosmic Frontier, Experimental High Energy Physics Research

\vspace{3mm}
\noindent
{\bf Topic Area Program Manager:} James Stone

\vspace{3mm}
\noindent
{\bf Year Doctorate Awarded:}  2002


\vspace{3mm}
\noindent
{\bf Number of Times Previously Applied:} 2

\vspace{3mm}
\noindent
{\bf PAMS Preproposal Number:} 857


\vspace{3mm}
\noindent
{\bf PECASE Eligible:} Yes



\newpage

\tableofcontents

\newpage

\title{Early Career: Measuring Dark Energy with Gravitational Lensing in 
the Dark Energy Survey}
\author{Erin Sheldon\\
{\normalsize Physicist}\\
\normalsize{Brookhaven National Laboratory}}
\date{}
\maketitle

\setcounter{page}{1}
\addcontentsline{toc}{section}{Abstract}
\begin{center}
\section*{Abstract}
\end{center}

We propose to study the properties of Dark Energy using data from the Dark
Energy Survey (DES), of which Erin Sheldon (ES) is survey ``builder'' with data
rights for himself and a postdoc.  This proposal has two aspects: development
of new methods to measure lensing effects from astronomical images and
interpretation of the lensing in terms of Dark Energy.

Dark Energy accelerates the expansion of the universe, dramatically altering
how the volume changes over cosmic time in comparison to a matter-only
universe.  Dark Energy also inhibits the growth of massive structures under
gravitational collapse.  The number of density of very massive objects such as
galaxy clusters, the extrema of the mass density field, is exponentially
sensitive to these effects.  Crucial to making use of this sensitivity is
knowing the masses of the galaxy clusters.  Using DES data, Sheldon and a
postdoc will use the gravitational lensing effect to infer these masses to
constrain cosmology.  This program is a natural continuation of earlier
measurements by Sheldon of cluster lensing in the Sloan Digital Sky Survey
(SDSS).  This grant will support a postdoc to participate in this analysis.

\begin{comment}
Dark Energy accelerates the expansion of the universe, dramatically increasing
the volume in comparison to a matter-only universe.  Dark Energy also inhibits
the growth of massive structures under gravitational collapse.  Thus the number
density of massive objects such as galaxy clusters as a function of cosmic time
is directly related to the properties of Dark Energy, in particular the
equation of state parameter $w=$pressure/density.  Critical to using the number
density to constrain cosmology is knowing the masses of the clusters. A
research associate and Sheldon will measure these masses using gravitational
lensing.  Using cluster counts, lensing and other complimentary probes, the DES
will measure $w$ to $\sim$3\%.  This program is a natural continuation of
earlier measurements by Sheldon of cluster lensing in the Sloan Digital Sky
Survey (SDSS).  This grant will support a research associate to lead this
analysis.
\end{comment}

Erin Sheldon is prototyping a promising new algorithm to measure lensing
effects from astronomical images.  In simulated imaging data, this method meets
the DES requirements for lensing measurement accuracy. While highly
computationally intensive, the algorithm is ideally suited to use graphics
processing units (GPUs). An implementation for GPUs already exists and huge
speed gains have been achieved over a highly optimized CPU code, making
analysis of the full DES possible with a moderate amount of computing
resources.   This grant will supply funds for this computing.
%In addition, Sheldon, in collaboration with Mike Jarvis of UPenn, is
%continuing to test and tune the de-facto algorithm and pipeline we have
%developed for DES.

DES has now entered the commissioning phase.  The DES survey proper will begin
in \surveyproper\ and will take data for five years. Sheldon and a research
associate will continuously process the data as it arrives to measure
gravitational shear, followed by analysis to constrain Dark Energy.  This
science analysis will most likely produce results in three stages: an early set
of results from the first year data, a second set of results after two or three
years when multi-epoch data are available, and a final set of results from the
full data set circa 2017.


\newpage
\addcontentsline{toc}{section}{Narrative}
\section*{Narrative}
\setcounter{section}{1}
\subsection{Introduction}

%The phenomenon of Dark Energy is one of the unsolved mysteries of our time.
The initial discovery of Dark Energy was made by studying the expansion history
of our universe.  According to General Relativity, a universe containing only
ordinary matter decelerates at late times under its own gravitation, but recent
studies of the brightnesses of distant supernovae indicate that our universe
has begun to accelerate \cite{Riess98,Perlmutter99}.  This can happen if there
is an exotic energy component in our universe with an equation of state
parameter $w$=pressure/density that is less than -1/3.

The observational consequences of a Dark Energy component in our universe are
numerous, but for our purposes the following are most relevant: 1) The
expansion history follows a form quite different from that of a matter only
universe, with a change in the sign of the acceleration at late times from
negative to positive \cite{Carroll92}.  2)  Unlike in a matter-only universe,
the growth history of massive structures through gravitation is no longer
determined solely by the properties of the mass density field and the value of
the present day expansion rate \cite{Haiman01}.  For example, without Dark
Energy, the number density of gravitationally collapsed structures of a given
mass at a given time in history is predictable essentially from the Hubble
expansion constant $H_0$ and low order statistics of the mass field such as the
mean of the density and the variance in the density as a function of scale
(ignoring baryons).  But in the presence of Dark Energy, the volume changes
over time in a dramatically different way; additionally the growth of
structures is slightly inhibited due to the acceleration.  These cumulative
effects from Dark Energy significantly alter the predicted number of massive
structures in a given volume of space.

Gravitational lensing is particularly well suited to studying this problem
(e.g. \cite{Kaiser98,Hu04}).  Light emitted by distant sources is bent as it
passes massive foreground structures, or ``lenses''.  The amount of bending
depends on the mass of the lens and the geometry of the lens-source-observer
system.  Thus, with lensing, one can infer properties of the mass field via the
lens masses. Furthermore, by studying lenses and sources from different times
in the history of the universe, one can use the geometrical dependence to infer
the expansion history.

Lensing is more appropriate for measuring masses in an cosmological context
than other techniques.  The traditional technique of using orbital calculations
and Kepler's laws to infer masses does not work because the timescales are too
long to characterize the orbits. Velocities can only be used in a statistical
way, and their use requires assumptions about the dynamical equilibrium of the
systems in question, which is often dubious on the physical scales of interest.
One can attempt to infer mass by counting luminous tracers such as stars, but
this is complicated by the ubiquitous presence of another mysterious substance:
Dark Matter.  Dark Matter dominates the mass density field, but because it is
collisionless it is distributed very differently from the luminous matter.
Often there are no luminous tracers in the relevant regions of space with which
to infer mass.  With lensing, to infer the mass one only needs enough
background sources with which to statistically measure the lensing signal.

Sheldon will use data from the Dark Energy Survey (DES, \cite{DESWhitePaper})
to perform gravitational lensing measurements and infer the properties of Dark
Energy.  Sheldon is a DES ``builder'', having worked steadily on the project
since its inception.  This designation brings perpetual data rights for
Sheldon, a postdoc and students.  Sheldon is a leader in the DES lensing
effort.  

Erin Sheldon's focus will be primarily on optimal measurement of the
gravitational shear induced in the shapes of galaxies by lensing. Shear is the
coherent distortion produced in the images of extended background sources by
the lensing effect, and has been studied in great detail over the last decade.
No method yet tested for measuring shear has proven accurate enough to meet the
DES survey requirements.  As described below, Sheldon has developed a new
technique to measure shear that shows promise to meet the DES requirements for
shear measurement accuracy.  This technique is highly computationally
intensive, but is also nearly perfectly suited to use of graphics processing
units (GPUs) which we have shown provide huge speedups relative to CPUs.

Under Sheldon's guidance, a postdoc will focus on using the shear to infer
statistics of the mass density field, namely the mass associated with galaxy
clusters and, more generally, the mass density fluctuations in our universe.
Both of these measurements will be highly sensitive to the properties of Dark
Energy.   As we will describe below, this is a natural extension of earlier
gravitational lensing measurements by Sheldon using data from the Sloan Digital
Sky Survey (SDSS).  Yet there much to learn in order to reach the full
potential of this method.

The rest of this narrative is divided as follows: In section \ref{sec:sdssold}
we describe previous research by Sheldon in gravitational lensing, which can be
thought of as a precursor for the DES work.  In section \ref{sec:des} we
describe the Dark Energy Survey (DES) and in section \ref{sec:deslensing} we
describe plans for lensing measurements using DES data.  In section
\ref{sec:gmix} we describe the new technique for shear measurement Sheldon is
developing, which shows promise to reach the required precision for DES. 
%In section \ref{sec:shapelets} we describe the ``de-facto'' pipeline we have
%developed over the past few years.  
Finally, in sections \ref{sec:resources} and \ref{sec:timeline} we describe the
personnel and resources needed to complete the work in this proposal and the
expected timeline.


\subsection{Previous Lensing Measurements in the SDSS} \label{sec:sdssold}

Using data from the SDSS \cite{York00}, Sheldon made highly precise
measurements of gravitational shear. Sheldon and collaborators have used these
measurements to estimate the total mass content (normal and dark) associated
with galaxies and clusters of galaxies
\cite{fis00,Sheldon04,SheldonLensing07,JohnstonLensing07,SheldonM2L07}.  These
high quality measurements are facilitated by the excellent data and processing
software of the SDSS, and our development of interpretational techniques that
can extract masses from complex statistical shear measurements
\cite{JohnstonInvert07}.

In these works we measured the shear from millions of background source
galaxies at various projected distances from foreground lenses, from which we
inferred the radial mass density profile.  Because the signal is very weak, we
additionally averaged the signal over many lenses.  This averaging, while in
principle diluting information about the individual lenses, has the positive
effect of averaging out line of sight projections and ``lumpiness'' in the
lenses, which complicates the interpretation.  This in turn facilitates the
extraction of accurate masses.  Figure \ref{fig:massngals} shows results from
\cite{SheldonLensing07,JohnstonLensing07}. For this plot we used as lenses
groups and clusters of galaxies.  Plotted is mean mass as a function of the
number of galaxies in the group.  

%\begin{figure}[ht]
\begin{figure}[p]
\centering
\includegraphics[scale=0.7]{mass-rich-plot.eps}
\caption{Mean mass as a function of the number of
galaxies in the group/cluster as measured by the PI from lensing in SDSS
data \cite{SheldonLensing07,JohnstonLensing07}. This calibration
is critical to measuring Dark Energy with galaxy clusters.\label{fig:massngals}}
\end{figure}




From measurements using galaxies as the lenses, we confirmed that there is an
enormous amount of unseen dark matter in galaxies, and that this dark matter is
in a ``dark halo'' that extends far beyond the concentrated bundle of stars at
the center of galaxies.  We have found similar results for groups and clusters
of galaxies; there is a pool of Dark Matter between galaxies in the group.
These measurements are completely consistent with the cold dark matter model.
A number of derived results have come from these basic measurement papers, in
which we have learned a great deal about the connection between the dark and
visible matter in galaxies and clusters, e.g.
\cite{RykoffLXM08,RozoScatter09,TinkerM2N2012}. 

Sheldon and collaborators have also used these measurements to estimate
cosmological parameters.  As stated in the introduction, the number density of
halos of a given mass is related to the mean mass density of the universe and
variance in the density.  In \cite{RozoCosmo09} we combined the counts of
galaxy clusters with our lensing mass estimates to constrain these cosmological
parameters.  Figure \ref{fig:omegasigma8} shows results from \cite{RozoCosmo09}
constraining the fractional mass density $\Omega_m$ (relative to the total
energy density) and the variance in mass density on 8 Mpc scales $\sigma_8$
relative to the mean. In \cite{TinkerM2N2012} we used the ratio of mass to
number density in clusters, combined with the large scale clustering of all
galaxies, to place further complimentary constraints on $\Omega_m$ and
$\sigma_8$.  All these results are consistent with one another and the Cold
Dark Matter theory.

\begin{figure}[p] 
\centering 
\includegraphics[scale=0.6]{s8_Om.ps}

\caption{Constraints on the fractional mass density of our universe $\Omega_m$
    (relative to the total energy density) and the variance in the density on 8
    Mpc scales $\sigma_8$, relative to the mean, as measured from SDSS data.
    These results \cite{RozoCosmo09} are derived by combining the counts of
    galaxy clusters with the mass calibrations from gravitational lensing as
    shown in Figure \ref{fig:massngals}
    \cite{SheldonLensing07,JohnstonLensing07}.  The cluster results break
degeneracies with other probes such as the cosmic microwave background (WMAP).
With DES we will study Dark Energy properties by extending these measurements
back in time.  \label{fig:omegasigma8}} 

\end{figure}


While powerful in themselves, these results are also very complimentary to
other measurements, breaking degeneracies in analyses of the Cosmic Microwave
Background \cite{KomatsuWMAPCosmo09}. 

Gravitational shear measurements are central to two of the primary goals of the
Dark Energy Survey (DES, section \ref{sec:des}).  The techniques we developed
in the SDSS are directly applicable to DES science, especially the study of
galaxy clusters as cosmological probes.  By extending the measurements backward
in time with the deeper DES data, we will learn about Dark Energy as well as
Dark Matter.


\subsection{The Dark Energy Survey (DES)} \label{sec:des}

The Dark Energy Survey (DES) is an optical, multi-band survey of 5000 square
degrees using the 4-meter ``Blanco'' telescope at the Cerro Tololo
Inter-American Observatory in Chile. A new camera is being built and the
telescope repaired and upgraded.  The DES will utilize gravitational lensing,
an optical cluster survey, supernovae, and galaxy clustering to constrain the
properties of Dark Energy.  Combining DES lensing measurements and DES optical
observations of galaxy clusters with observations by the South Pole Telescope
(SPT, \cite{SPT04}) of the same galaxy clusters, greatly enhances the
constraining power.  These combined methods will constrain the Dark Energy
equation of state parameter $w$ to better than 3\%; an important component of
this overall constraint will be the cluster lensing measurements led by
Sheldon.  First light occured in \commissdate, and the survey will run for five
years.  DES operations are funded in part by the U.S. Department of Energy. 


The SPT will use the Sunyaev-Zel'dovich (SZ) Effect \cite{Birkinshaw99}, the
Compton up-scattering of light from the cosmic microwave background by the hot
gas in galaxy clusters, to find a complete sample of clusters to high redshift.
This selection of this cluster sample is complimentary to the DES optical
cluster selection in that it is expected to be nearly independent of the
distance of the cluster from the observer.  As with DES, the goal of the SPT is
to use these clusters to probe the growth of structure, and the volume of
space, as a function of time in order to constrain Dark Energy.  Since it is
the number density of clusters of a given mass that is sensitive to Dark
Energy, an important part of each cluster survey will be the calibration of the
mass-observable relationship via lensing.  In \S \ref{sec:deslensing} we will
describe in detail our plans for measuring this relationship using DES optical
data.

In addition to galaxy clusters, the DES will use a number of other probes to
constrain Dark Energy properties.  These include two other lensing probes:
Shear-shear correlations as a function of scale and the cross-correlation
between shear and known objects as a function of scale.  Data are shared
between cluster mass measurements and these probes, but because the correlation
functions cover a much larger range of scales, they are complimentary.  There
is also a Supernova program that, while less constraining by itself, breaks
degeneracies between certain cosmological parameters.  

Table \ref{table:constraints} contains forcasted constraints on $w$ for various
techniques employed by DES \cite{DESWhitePaper}.  These forecasts are for DES
and SPT data alone; combining with other data, for example from cosmic
microwave background measurements from the Planck satellite
\cite{PlanckBluebook}, can significantly increase the precision of certain
probes.

\begin{deluxetable}{ll}
\tablecaption{Projected DES Constraints on Constant $w$ Dark Energy Models.
\label{table:constraints}}
\tablewidth{0pt}
\tablehead{
	\multicolumn{1}{l}{Method} &
	\colhead{$\sigma_w$}
}
\startdata
Clusters &  \\
~~~Abundance & 0.13  \\
~~~with WL Calibration & 0.09 \\
Weak Lensing & \\
~~~Cosmic Shear (CS) & 0.15  \\
~~~Galaxy/Cluster-shear(GS) + Angular Clustering(AC) & 0.08  \\
~~~CS + GS + AC & 0.03  \\
Angular Clustering of Galaxies & 0.36 \\
Supernovae Ia & 0.34 \\
\enddata
\end{deluxetable}

\subsection{Overview of DES Activities}

The work on DES has two primary aspects of interest for this proposal:
development of techniques to measure gravitational shear and science analysis
of the shear in terms of Dark Energy.  Sheldon will focus primarily on
developing shear measurement techniques, including a promising new technique
and a fast implementation using graphics processing units.  Sheldon will guide
a research associate to analyze these measurements to extract cosmological
parameters.  In the following sections we will describe the current state of
this research and planned activities.

\subsection{Lensing Analysis of DES Data} \label{sec:deslensing}

\subsubsection{Science Analysis}

As described in the introduction, the cosmological information from clusters is
primarily in the number density of objects with a given mass as a function of
time, especially the most massive objects such as galaxy clusters.  Clusters
are identified not by their mass but by other indicators, such as the number of
galaxies in a cluster or the SZ effect.  The SZ effect and the number of
galaxies are both correlated with mass, but that correlation must be measured
by a secondary method.  As described in the introduction, lensing is the best
method for doing this.  

As described in \S\ref{sec:sdssold}, in our studies of SDSS lensing we
developed analysis techniques to calibrate the mass-observable relation of
clusters (Figure \ref{fig:massngals}).  Using this calibration in conjunction
with the number density we have inferred cosmological parameters (Figure
\ref{fig:omegasigma8}).  These techniques are limited only by our understanding
of the systematics and characterization of the cluster selection process.  The
volume and depth of DES is sufficiently large to perform measurements
equivalent to the SDSS study, but separately in many bins of cosmic time.  Time
dependent measurements will allow us to extend our cosmological analysis to
constrain the properties of Dark Energy.

There is much room for innovation in this field: the sensitivity of DES will be
such that small systematic effects will become important. Some such issues are:
offsets between the identified galaxy cluster position and the true center of
mass; measurements errors due to object overlap in crowded regions at cluster
centers; errors introduced by distance uncertainties in the cluster catalog and
source catalog; bias in the orientation of clusters introduced by the cluster
finding procedure.  At the same time, the great sensitivity of DES imaging will
create the potential to explore entirely new lensing-based probes.  What we
learn from DES, the largest survey to date, will be highly influential for
future surveys such as LSST.

In contrast with cluster lensing measurements, the ``cosmic shear'' is the
correlation of shears across the sky independent of the location of foreground
structures.  Since the shear is related to mass, the cosmic shear can be used
to infer statistics of the underlying mass distribution, the evolution of which
is directly related to the properties of Dark Energy.  Because the signal need
not be modeled in terms of cluster halos, the interpretation of cosmic shear
can be simpler than cluster lensing.  However, the measurement involves
directly correlating shears from many sources as a function of their separation
on the sky, which can propagate systematic errors directly into the
measurement. Thus cosmic shear and cluster lensing are quite complimentary.

Table \ref{table:constraints} contains a list of DES probes and their power to
constrain the equation of state $w$.  If the desired accuracy is achieved,
cluster lensing (clusters with WL calibration), which is begin led by Sheldon,
is the most sensitive probe.

Erin Sheldon will guide a research associate in using shear measurements to
calibrate the masses of the galaxy clusters used in the cosmological analyses.
We will also work with other DES collaborators on the measurement of cosmic
shear.  These are critical components of the DES mission.  We expect major
results to appear after year one based on the single epoch data, year 2-3 after
multi-epoch data becomes available, and at survey end with the final data set.

In our forecasting of cosmological parameter constraints, we have assumed we
can meet a certain level of lensing shear measurement accuracy.  This analysis
is ultimately limited by how accurately we can measure galaxy ellipticities and
remove the effects of the point spread function.  In the next section we will
address the second major part of this proposal: a new method to measure shear
and a fast implementation on graphics processing units.

\subsection{New Lensing Shear Measurement Technique Using Gaussian Mixtures}
\label{sec:gmix}

The weak gravitational shear effect discussed in the previous sections is a
very small distortion of the images of galaxies by foreground mass
distributions due to the bending of light.  This small distortion produces
correlations in the ellipticities of the background galaxies, which can only be
measured statistically.  In principle all one needs is enough galaxies to
extract this signal from the noise.  However, the telescope optics and
atmosphere distort and blur the images of galaxies, and this also produces
correlations in the ellipticities of galaxies; the combined effect is generally
referred to as the point spread function (PSF). The primary affect is a
convolution, which means the effect is more significant for smaller galaxies.
Most detected galaxies are of order the size of the PSF, and the effect is
typically at a level much larger than the lensing effect we wish to measure.

Many methods have been developed to measure and remove the effect of the PSF to
produce unbiased ellipticity measurements, e.g.
\cite{ksb95,Bern02,Miller07,Melchior11} and there are many more.  No method yet
tested has proven accurate enough to meet the DES survey requirements when
applied to realistic galaxy populations:  for DES to meet the projected
constraints on dark energy, the fractional error on the shear cannot exceed
0.004.

Erin Sheldon has developed a new technique that shows promise to meet the DES
requirements.  The method uses mixtures of Gaussians to represent the galaxies
and PSF, and bayesian techniques to control noise bias.  This model has enough
flexibility to represent the canonical galaxy types; this has recently been
independently verified \citep{HoggGMix12}.  One result of our recent research
is that many fewer Gaussians are required than used in \cite{HoggGMix12} when
the object is of order the size of the PSF; three is enough.  
%When also
%demanding the Gaussians are co-centric and co-elliptical the parameter space is
%reduced even further.  

It is tempting to just use the canonical galaxy profiles (exponential disks and
\devprof\ ``\devauc'' profiles) to fit for the ellipticity, but these are
computationally intensive to generate when convolving with a PSF and the pixel
response.  Using Gaussians for both PSF and galaxy greatly increases the speed
because the convolutions can be done analytically.   Furthermore, the Gaussian
mixture model can adapt to profiles that differ from the canonical models.

Other features of the method are more generic and are designed to mitigate the
effects of noise.  Noise is known to produce bias in the shear measurement when
using either maximum likelihood \cite{Refreg12} or the expectation value
derived from a full exploration of the likelihood \cite{Miller12}.  This can be
mitigated using the techniques in \cite{Miller07,Miller12}. These techniques
involve measuring the full likelihood surface and applying prior information on
the ellipticity distribution of the galaxy population as a whole\footnote{In
\cite{Miller12} a prior on size is applied, but we have found this unnecessary
if a proper \snsize\ cut is made}.
Corrections can then be derived to mitigate bias introduced by the priors and
noise.  Sheldon has implemented these techniques using a Monte Carlo Markov
Chain technique, and the code is publicly available (although the use of priors
is clearly described in \cite{Miller07}, that author's code is proprietary).

The results from simulations indicate the method can potentially meet DES
requirements.  The shear bias as a function of signal-to-noise ratio \snsize\
is shown in \ref{fig:getgdt}.  Results for simulated exponential disk galaxies
are shown in the left panel.  Galaxies with \devprof\ profiles are shown in the
right panel.  The DES requirement is shown as the gray band.  The DES
requirements for shear accuracy are met for galaxies with \snsize\ $\geq$
\sncut.  

The value \snsize\ is the signal-to-noise ratio of the measured size
$T=\sigma^2$ of the object.  The \snsize\ is a good indicator because galaxies
with different profiles have similar shear bias at fixed \snsize.  In contrast,
the weighted S/N of the flux is not a good indicator: objects at fixed weighted
flux S/N have different shear bias.  This effect is not completely understood,
but it is true that size and ellipticity are degenerate in these fits, and a
very poorly constrained size results in more scatter in the ellipticity and
more noise bias.

\begin{figure}[t]
\centering

\includegraphics[scale=0.4]{mcbayes-get03r01r02r03r04r05-yr-0.030-0.030-frac.eps}
\includegraphics[scale=0.4]{mcbayes-gdt03r01r02r03r04-yr-0.030-0.030-frac.eps}

\caption{Fractional shear bias as a function of the signal-to-noise ratio
    \snsize\ of the object scale size.  The simulated galaxies are comparable
    in size to the PSF.  The PSF represents atmospheric turbulence.  Left
    panel: Simulated exponential disk galaxies fit by a Gaussian mixture.  A
    prior on the distribution of ellipticities has been applied.  The gray band
    is the DES requirement on the shear bias. Right panel: Same as the left
    panel but for simulated \devprof\ galaxies.  These simulations indicate a
    cut on \snsize\ $\geq$ \sncut\ removes the most biased sample.
\label{fig:getgdt} }

\end{figure}

\subsubsection{Speeding Up the Computations with GPUs} \label{sec:gmix:gpu}

The corrections discussed in the previous section require measurement of the
full likelihood surface, and are thus very computationally intensive.  Using
the Gaussian mixture model provides a fast and flexible model for the
necessarily convolutions, but the large number of likelihood evaluations
require of order 1 second per galaxy on a CPU (2.7 GHz intel). The requirements
to process the full DES will be enormous: the expected 500 million galaxies
will be observed on $\sim$ 10 separate exposures, which would require $\sim$
1.4 million cpu hours to process. Note the CPU computation is highly optimized,
including a fast approximate exponential function five times faster than the
standard. 

We have found significant gains in speed by using a graphics processing unit
(GPU).  This problem is particularly suited to GPU processing because the image
must be uploaded only once, and then thousands of likelihood evaluations are
made on the GPU. Only a single number, the likelihood, must be returned by the
calculations, minimizing the overhead of moving data back and forth from the
GPU memory.  We have found that each likelihood calculation is faster by a
factor of $>$\speedupnum\ compared to the CPU code, primarily due to
parallelization of the pixel rendering of the model; each pixel can be rendered
in parallel.

Further speedup was obtained by parallelizing the Markov chain.  We use an
affine invariant Markov chain\cite{GoodmanWeare10} with twenty separate
``walkers'' that are evaluated independently at each step.  The total speedup
is about a factor of \overallspeedup.  This is somewhat less than the 20x10=200
possible speedup, presumably due to data transfer inefficiencies.  This gain of
\overallspeedup\ may seem exceptional, but is due to the extreme
parallelization we can achieve.  This parallelization is made possible by the
small image dimensions for a typical galaxy (25x25) and independence of the
likelihood evaluations for each walker.

We can also make use of multiple GPUs attached to a single machine; we shall
propose attaching two.  Of cource there are also typically multiple CPUs per
system.  Comparing a 2 GPU system to a 12-core CPU system, the total speedup is
2*170/12 = 28.

These increases in speed come at a moderate increase in cost over typical
``farm'' compute nodes.  Adding two high performance GPUs to a typical node
increases the cost by about 70\%. In section \ref{sec:computing} we will
outline the moderate cost to build a sufficient set of machines to process the
DES data.

{\it
    
Notes:  All the code was written in the OpenCL and the C language by Erin
Sheldon and is publicly available.  Some more time can be spent tuning the GPU
implementation. On the other hand, the ultimate bottleneck will be memory
bandwidth limitations, which these tests may already have met.   More walkers
could be used for better parallelization, but the tradeoff is a more costly
burn-in for the Markov chain, as the total burn-in does not decrease inversely
with the number walkers.  A ``high-end'' graphics card is required to get this
speedup; we tested an Nvidia 2050. The comparison CPUs are 2.7 GHz intel Xeon
processors.  The number of likelihood evaluations needed is the number of
walkers times (burn-in + steps).  For higher S/N objects, a burn-in of about
400 steps per walker is needed, followed by 200 steps.  A large number is
needed after burn-in to measure the tails of the distribution, which are
important.  A high performance distributed file system is needed to get the
required data throughput: the cosmology group at BNL uses the Hadoop
Distributed File System, which provides sufficient throughput by allowing jobs
to run on the same machine holding the data; input/output from disk is only
about 10\% overhead.  }

\subsubsection{Future Work on Gaussian Mixtures}

There is much future work to be done on the Gaussian Mixture method.  We must
determine the priors on the ellipticity distribution from real data.  This is a
topic of research because, at faint magnitudes, it is difficult to determine
these distributions, especially for \devprof\ galaxies\cite{Miller12}.  Another
area of focus will be testing the method on more complex simulated galaxy and
PSF models.  Finally, the ultimate test is real data and we expect getting the
method to work optimally on real data will require the most effort.


\begin{comment}
\subsection{Shear Measurement Using Shapelets}
\label{sec:shapelets}

In addition to the Gaussian mixture method described in section \ref{sec:gmix},
Mike Jarvis of the University of Pennsylvania and I have implemented a full
shear pipeline based on the shapelets technique\cite{Bern02}.   The accuracy
does not yet meet DES requirements, but this pipeline is mature in the sense
that it can now run fully autonomously on the data and is fast and stable.
Tuning of the pipeline to increase accuracy is being led by Jarvis and is
showing promise.   The ideas for reducing bias, outlined in section
\ref{sec:gmix}, can in principle be applied to this technique as well.

This pipeline can make full use of the multi-epoch DES data where the sky is
observed many times.  This code has been tested on simulated DES data and will
continue to undergo heavy testing and refinement throughout the commissioning
phase.  

The processing framework was implemented by Sheldon.  This framework is
``pluggable'' so that other codes can be plugged in readily and can make use of
the infrastructure built around the shapelets code.  I plan to plug in the
Gaussian mixture code as well as 2-3 other pipelines developed within DES.

Note the shapelets pipeline, as it is implemented currently, gains less from a
GPU implementation than the Gaussian mixtures because it is a maximum
likelihood technique and so requires relatively few evaluations.  That said,
significant gains should be possible.  And, if the bias-correction schemes
discussed in section \ref{sec:gmix} are implemented for shapelets, then greater
gains can be had using GPUs.
\end{comment}

\subsection{Personnel and Resources} \label{sec:resources}

\subsubsection{Research Associate}

This grant will be used to fund our current postdoc, Andres Plazas, for the
first two years of the award. Plazas is already working on DES and it makes
sense for him to continue under this funding.  A second postdoc will begin in
the third year after Plazas' departure.  

The work of these postdocs will focus on high-level science analysis.  In
particular, we have carved out a niche in the field of cluster lensing and
cosmology, and we expect the postdocs will primarily work on that analysis.  As
outlined in section \ref{sec:sdssold}, this involves interpreting the shear in
terms of mass, and incorporating this into an overall measurement of cosmology.
These analyses will be important and will have high scientific impact, as
cluster lensing and cosmology is one of the primary probes used in DES.

Other analyses are also available at the postdocs discretion; the DES data set
will present an opportunity to explore new lensing-based probes.  Postdocs well
versed in computation may help with the computationally intense, GPU-based,
Gaussian Mixture technique outlined in section \ref{sec:gmix}.

\subsubsection{Computing} \label{sec:computing}

In section \ref{sec:gmix} we described a promising new shear measurement
technique based on Gaussian mixtures, and a fast implementation on graphics
processing units (GPUs).  Here we will give justification for funding a
moderate computing purchase with this grant.

We will have a lot of data to process.  We are commissioning the DES survey at
the time of this writing (\commissdate) and will run for five years, generating
about a petabyte of data in total.  For lensing we will process all the reduced
images, including analysis of multiple epochs for each pointing.  This subset
represents about 130TB of data.

The NSF funded Dark Energy Survey data management team will process the data
using a selected lensing pipeline approximately yearly.  However, the Gaussian
mixture pipeline is still in heavy development and will require many runs on
the data and simulations to find and fix bugs.  A turnaround time more like
$\sim$2.5 weeks on the full data set is desirable.   This will facilitate
development in response to data-driven needs as well as sufficient computing
for any simulation tests we require.

%Note the DES collaboration may also have some computing available to run the
%pipelines, but these systems will not be GPU based so the computationally
%intensive Gaussian mixture code is not a good use of that resource.

As described in section \ref{sec:gmix:gpu}, we want to process all 500 million
galaxies, each on 10 separate exposures, in this time of 2.5 weeks. Time per
galaxy is of order 1 second on a 2.7GHz Intel CPU.  In section
\ref{sec:gmix:gpu}, we showed that with two GPUs we got a factor of
\overallspeedup\ speedup as compared to a single CPU.  With these speedups we
we would require only $\sim$ 10 nodes, each with two GPUs, to reach our goal. 
%We round this up to 10 nodes for two reasons: 1) the Gaussian mixture
%algorithm is under development and computational requirements may increase and
%2) it is more cost-effective and efficient for data throughput to spread the
%required disk to more machines.  

What type of systems should these be?  We desire fairly large memory on the
system in order to hold data from all exposures in memory for a full ``coadd
tile''. A coadd tile is the unit of association used when combining exposures
and covers several tens of pointings, (with about ten exposures per pointing).
Tests indicate 32GB is sufficient to avoid input/output overhead.  We need
130TB in the distributed file system, so about 13TB per machine (we can in
principle do without redundancy as the data are readily available for download
from the DES collaboration).  Recent purchases for the Atlas experiment at BNL
are very similar to this and we project a cost of approximately \$5,200.  The
addition of two high-performance GPUs (e.g. Nvidia 2050) adds another $\sim$
\$3,600.  The total cost for ten machines is \$88,000.  Note overhead and
slight yearly escalation are also included in the budget.  These can be
purchased over the five year survey, in stride with data acquisition.  There is
potential for increase in speed per GPU, but this is difficult to predict
currently as scientific GPUs are a new technology.  We prefer to quote the
current performance profile.

Note these prices include bulk discounts from purchasing along with other
experiments through the RHIC-Atlas Computing Facility at BNL.  

\input{cost.tex}

\subsubsection{Salary and Travel}

It is required that DOE employees pay their salary from this grant, so 100\% of
Erin Sheldon's salary and overhead for five years is requested.  We also request
support for a research associate for five years.

The remaining expenses beyond salary and computing are primarily for travel.
The DES collaboration is multi-national and Erin Sheldon and a postdoc will
most likely travel abroad to the collaboration meetings.  Sheldon plans to
spend three weeks each summer at a workshop in the US, such as those held at
Aspen and Santa Fe.  Further travel will involve attending a few conferences
per year across the US for both Sheldon and postdoc.  Erin Sheldon will also
make regular trips to the University of Pennsylvania to collaborate with fellow
DES weak lensers Mike Jarvis and Bhuvnesh Jain.

\clearpage
\newpage
\subsection{Timeline} \label{sec:timeline}

This is an outline of the activities for the five year extent of the award.
For reasons of continuity, we will begin the timeline in Fall 2012.  It is
assumed that the primary collaborators on these activities are Erin Sheldon of
BNL and a postdoc, or research associate.

\begin{itemize}

\item {\bf Fall 2012-Spring 2013} Process and evaluate commissioning data as it
    arrives.  Continued testing of the weak lensing pipelines on simulated
    data.

\item {\bf Late Spring through end of 2013} Survey proper begins; process data
    as it arrives.  Re-process data as bugs are found and algorithms improve.
    Processing using Gaussian mixtures at BNL on GPU cluster.  Begin science
    lensing analysis.

\item {\bf 2014} Continue processing data and testing pipelines.  Publication
    of first year DES measurements for galaxy and cluster-mass correlations.

\item {\bf 2015} Continue processing data as it arrives.  Continue improving
    algorithms.   First publications using multi-epoch data in 2014.

\item {\bf 2016-2017}  Activities should continue as before until survey end.
    Processing data as it arrives, incrementally improving the data pipelines
    and analysis codes.  Analysis methods will evolve, especially as the final
    data are in hand and there are many epochs with which to work.   There will
    be intermediate publications based on this data.

\item {\bf Post survey} A re-processing of all data through the final pipelines
    and final analysis of the full dataset to extract cosmological information.


\end{itemize}







\newpage
\addcontentsline{toc}{section}{Appendix 1: Biographical Sketch}
\section*{Appendix 1: Biographical Sketch}

%\setlength{\oddsidemargin}{-0.1in}
%\setlength{\evensidemargin}{-0.1in}
%\setlength{\textwidth}{6.7in}
%\setlength{\topmargin}{-0.25in}
%\setlength{\textheight}{9.0in}

\newcommand{\tsp}{\vspace{0.1cm}}
\newcommand{\isp}{\vspace{0.3cm}}
\newcommand{\ssp}{\vspace{0.4cm}}


{\Large {\bf Erin Sheldon}}
\tsp
%
% Address information.
%

\noindent
Bldg 510

\noindent
Brookhaven National Laboratory

\noindent
Upton, NY 11973

\noindent
(631) 344-3117

\noindent
erin.sheldon@gmail.com

\newcommand{\myshorttab}{0.5in}
\newcommand{\mytab}{0.75in}
\newcommand{\mylongtab}{1.25in}

\vspace{0.4cm}
\noindent
\makebox[1.25in][l]{{\large \bf Education and Training}}{}
\vspace{0.2cm}
\newline
\makebox[\mytab][l]{}{Postdoctoral Fellow, Center for Cosmology and Particle Physics}
\newline
\makebox[\mylongtab][l]{}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{\bf New York University}}
    \hfill
    \makebox[1in][r]{{\small \it Sep 2005--Sep 2008}}
\newline
\makebox[\mytab][l]{}{Postdoctoral Fellow, Kavli Institute for Cosmological Physics}
	\newline
\makebox[\mylongtab][l]{}{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~{\bf University of Chicago}}
	\hfill
	\makebox[1in][r]{{\small \it Aug 2002--Aug 2005}}
\newline
\makebox[\mytab][l]{}{Research Assistant, {\bf FNAL}}
	\hfill
	\makebox[1in][r]{\small \it Jun 1998--Sep 1998}
\newline
\tsp
\noindent
\makebox[\mytab][l]{}{Ph.D., {\bf University of Michigan}}
    \hfill
    \makebox[1in][r]{\small \it Sep 1997--Aug 2002}
\newline
\tsp
\makebox[\mytab][l]{}{B.S., {\bf University of Missouri}}
\hfill
\makebox[1in][r]{\small \it Aug 1992 -- May 1997}


%
% Experience...
%

\vspace{0.4cm}
\noindent
\makebox[1.25in][l]{{\large \bf Experience}}{}
\newline
\vspace{0.2cm}
\makebox[\mytab][l]{}{Physicist, {\bf Brookhaven National Laboratory}}
\newline
\makebox[1.25in][l]{}{}
        \hfill
        \makebox[1in][r]{{\small \it Jan 2012--present}}
\newline
\vspace{0.2cm}
\makebox[\mytab][l]{}{Associate Physicist, {\bf Brookhaven National Laboratory}}
\newline
\makebox[1.25in][l]{}{}
        \hfill
        \makebox[1in][r]{{\small \it 2010--2012}}
\newline
\vspace{0.2cm}
\makebox[\mytab][l]{}{Assistant Physicist, {\bf Brookhaven National Laboratory}}
\newline
\makebox[1.25in][l]{}{}
        \hfill
        \makebox[1in][r]{{\small \it Sep 2008--2010}}




\vspace{0.3cm}
\noindent
\makebox[1.25in][l]{{\large \bf Graduate and Postdoctoral Advisors and Advisees}}{}
\vspace{0.2cm}
\newline
\makebox[\myshorttab][l]{}{{\large \bf Advisors}}
\newline
\makebox[\mytab][l]{}{Postdoctoral Advisor: David Hogg, New York University}
\newline
\makebox[\mytab][l]{}{Postdoctoral Advisor: Josh Frieman, University of Chicago}
\newline
\makebox[\mytab][l]{}{Thesis Advisor: Timothy McKay, University of Michigan}
\newline
\makebox[\myshorttab][l]{}{{\large \bf Advisees}}
\newline
\makebox[\mytab][l]{}{Postdoc, BNL: Zhaoming Ma}
\newline
\makebox[\mytab][l]{}{Postdoc, BNL: Andres Plazas}


%\noindent
%\makebox[1.25in][l]{}
%\parbox{5.40in}{
%Ph.D., Physics\newline
%Thesis advisor: Prof. Timothy McKay\newline
%Title of thesis: ``Galaxies, Luminosity, and Mass: Gravitational Lensing Measurements of the Correlation between Dark and Luminous Matter''
%}




\newpage
\vspace{0.2in}
\noindent
\newline
\newline
{\Large {\bf Selected Publications for Erin Sheldon} }
\newline
Note Appendix 3 holds the references for the narrative. 
\vspace{4mm}

\begin{tabular}{p{3mm} p{5.5in}}

1 & E.~S. {Sheldon} et~al.
\newblock {Photometric Redshift Probability Distributions for Galaxies in the SDSS DR8}.
\newblock {\em \apjs}, 201 32, August 2012. \\[6pt]

2 & J. {Tinker}, E.~S. {Sheldon} et~al.
\newblock {Cosmological Constraints from Galaxy Clustering and the Mass-to-Number Ratio of Galaxy Clusters}.
\newblock {\em \apj},  745:16 January 2012. \\[6pt]

3 & E. {Rozo} et~al.
\newblock {Cosmological Constraints from the Sloan Digital Sky Survey maxBCG Cluster Catalog}.
\newblock {\em \apj}, 708:645-660, January 2010. \\[6pt]

4 & E.~S. {Sheldon} et~al.
\newblock {Cross-correlation Weak Lensing of SDSS Galaxy Clusters III:
  Mass-to-light Ratios}.
\newblock {\em \apj}, 703:2232-2248, October 2009. \\[6pt]

5 & D.~E. {Johnston}, E.~S. {Sheldon}, et~al.
\newblock {Cross-correlation Weak Lensing of SDSS Galaxy Clusters II: Cluster
  Density Profiles and the Mass--Richness Relation}.
\newblock {\em arXiv:0709.1159}, September 2007. \\[6pt]

6 & E.~S. {Sheldon} et~al.
\newblock {Cross-correlation Weak Lensing of SDSS Galaxy Clusters I:
  Measurements}.
\newblock {\em \apj}, 703:2217-2231, October 2009. \\[6pt]

%5 & B.~P. {Koester} et~al.
%\newblock {A MaxBCG Catalog of 13,823 Galaxy Clusters from the Sloan Digital
%  Sky Survey}.
%\newblock {\em \apj}, 660:239--255, May 2007.\\[6pt]

7 & E.~S. {Sheldon} et~al.
\newblock {The Galaxy-Mass Correlation Function Measured from Weak Lensing in
  the Sloan Digital Sky Survey}.
\newblock {\em \aj}, 127:2544--2564, May 2004.\\[6pt]

%7 & T.~A. {McKay}, E.~S. {Sheldon}, D.~{Johnston}, E.~K. {Grebel}, F.~{Prada},
%  H.-W. {Rix}, N.~A. {Bahcall}, J.~{Brinkmann}, I.~{Csabai}, M.~{Fukugita},
%  D.~Q. {Lamb}, and D.~G. {York}.
%\newblock {Dynamical Confirmation of Sloan Digital Sky Survey Weak-lensing
%  Scaling Laws}.
%\newblock {\em \apjl}, 571:L85--L88, June 2002.\\[6pt]

8 & T.~A. {McKay}, E.~S. {Sheldon}, et~al.
\newblock {Galaxy Mass and Luminosity Scaling Laws Determined by Weak
  Gravitational Lensing}.
\newblock {\em ArXiv Astrophysics e-prints}, August 2001.\\[6pt]

9 & E.~S. {Sheldon} et~al.
\newblock {Weak-Lensing Measurements of 42 SDSS/RASS Galaxy Clusters}.
\newblock {\em \apj}, 554:881--887, June 2001.\\[6pt]

10 & P.~{Fischer}, T.~A. Mckay, E.~S. Sheldon, et~al.
\newblock {Weak Lensing with Sloan Digital Sky Survey Commissioning Data: The
  Galaxy-Mass Correlation Function to 1 Mpc}.
\newblock {\em \aj}, 120:1198--1208, September 2000.

\end{tabular}

\ssp
\ssp
\noindent
\parbox[l]{1.25in}{{\bf Synergistic \\ Activities}}
\parbox[t]{5.40in}{
Elected {\bf Builder} of the Dark Energy Survey \hfill {\small 2011} \newline
Elected {\bf Architect} of the Sloan Digital Sky Survey III \hfill {\small 2011} \newline
}

\newpage

\vspace{0.2in}
\noindent
\newline
\newline
{\Large {\bf Selected Collaborators} }
\newline

\noindent
\input{collab.tex}


\newpage
\addcontentsline{toc}{section}{Appendix 2: Current and Pending Support}
\section*{Appendix 2: Current and Pending Support}

Dr. Erin S. Sheldon is currently supported as a physicist at Brookhaven
National Laboratory at 100\%.  Pending project approval, support will be
redirected to this grant {\bf as required for laboratory employeees}.  There is
no other support pending.

\vspace{5mm}
\noindent
Investigator: Erin Sheldon\newline
Source of Support: DOE Office of Science\newline
High Energy Physics\newline
FWP: PO104\newline
Period Covered: 10/1/12-9/30/13\newline
Man Months: 12 months

\begin{comment}
\begin{table}[h]
\begin{center}
\begin{tabular*}{0.85\textwidth}{ll}
B\&R \#YN010000  & \parbox[t]{\textwidth}{BNL Laboratory Directed 
   Research \& Development \\ Award} \\
                 & LDRD 10-45 (FY 2010 -– FY 2012) \\
                 & Astrophysics \& Cosmology Initiative \\
                 & 100\%
\end{tabular*}
\parbox{0.85\textwidth}{\caption{{\bf Current Funding}: Program Development 
\& Lab Directed Research \& Development \label{table:support}}}
\end{center}
\end{table}
\end{comment}


\newpage
\addcontentsline{toc}{section}{Appendix 3: Bibliography for Narrative}
\renewcommand{\refname}{\section*{Appendix 3: Bibliography for Narrative}\label{app:bib}}
\bibliographystyle{unsrt}
\bibliography{astroref}
\vspace{5mm}
\noindent
{\bf Key:} {\it AJ} is The Astronomical Journal, {\it ApJ} is The 
Astrophysical Journal and {\it MNRAS} is The Monthly Notices of the Royal
Astronomical Society.






\newpage
\addcontentsline{toc}{section}{Appendix 4: Facilities and Other Resources}
\section*{Appendix 4: Facilities and Other Resources}

We plan to acquire a moderate amount of computing.  The housing, power and
cooling, administration, and maintenance for these computers will be provided
by the RHIC-Atlas Computing Facility at Brookhaven National Lab at no
additional cost to this experiment.	The tests we performed on GPUs occured on a
machine in the RACF set aside for prototyping.  The engineers who set up this
system can lend expertise to tuning the new GPU systems purchased with this
grant.

\newpage
\addcontentsline{toc}{section}{Appendix 5: Equipment}
\section*{Appendix 5: Equipment}

The equipment {\bf currently} available to this project is shared time on a set
of 34 compute nodes and a file server purchased in 2009-2012.  
%The file server holds 40 TB. Nine of the compute nodes are 12 core with 32GB
%ram, twelve are 12 core with 48GB ram, and the rest are 8 core 32GB ram
%systems.  These are housed at the RHIC-Atlas Computing Facility at BNL.  
Because these are a highly over-subscribed, shared resource, only a fraction of
the resources can be dedicated to this project.

% this is a pdf insert
\newpage
\addcontentsline{toc}{section}{Appendix 6: Other Attachments}
fake for letter

\end{document}
